# -*- coding: utf-8 -*-
"""ML  Parte II

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1foSio6NJTmtE7forIu-uvaO2VzE58suw

# **Carregar o dataset**
"""

import pandas as pd # importando o pandas para manipularmos o dataset
import numpy as np # biblioteca para manipulação de vetores e matrizes grandes além de outras manipulações de dados de larga escala
import seaborn as sns # importando o Seaborn para visualizar o comportamento dos dados
import matplotlib.cm as mcm # biblioteca para mostrar gráficos (espeficamente uma parte para cores)
import matplotlib.pyplot as plt # importando o Matplotlib para o elbow method
import pandas_profiling

from pandas_profiling import ProfileReport # importando o pandas-profiling para fazer o profile do dataset
from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV# utilizado para o split entre treinamento e teste
from sklearn.neighbors import KNeighborsRegressor # KNN para regressão
from sklearn.neighbors import KNeighborsClassifier # utilizado para treinar o KNN
from sklearn.linear_model import LinearRegression, LogisticRegression # Regressão linear, utilizado para treinar um modelo de classificação (regressão logística - apesar do nome é para problemas de classificação)
from sklearn.svm import SVR # SVM para regressão
from sklearn.decomposition import PCA # PCA como aprendizagem não-supervisionada
from sklearn.preprocessing import RobustScaler, OrdinalEncoder, OneHotEncoder # utilizado para que todas as entradas estejam na mesma escala numérica
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import * # importando todas as funções específicas de seleção de atributos do scikit-learn
from sklearn.decomposition import * # importando todas as funções específicas para a extração de atributos do scikit-learn
from sklearn.cluster import * # importando todas as funções específicas para o agrupamento
from sklearn.ensemble import * # importando vários ensembles para que possamos testá-los posteriormente
from sklearn.pipeline import Pipeline # utilizado para criar pipelines
from sklearn.metrics import f1_score, make_scorer # utilizado para calcular a performance dos pipelines
from lightgbm import LGBMClassifier # utilizado para treinar o LightGBM
from sklearn import set_config # utilizado para mostrar os passos do pipeline de forma visual
from sklearn.impute import SimpleImputer

set_config(display='diagram') # forçando para que os passos do pipeline sejam mostrados em visual

#seleção do dataset
df_nba = pd.read_csv('/content/nba_stats.csv') #carrega o dataset do diretorio 
df_nba.describe() #mostra destacaria os dados
#display(df_nba.columns)

"""# **Divisão do dataset**"""

# criando uma cópia do dataset para fins de teste
df_nba_copy = df_nba[~pd.isnull(df_nba["WinShares"])].copy()
df_nba_copy

# split entre treinamento e teste
X_train, X_test, y_train, y_test = train_test_split(df_nba_copy.drop('WinShares', axis=1), # aqui informamos os atributos
                                                    df_nba_copy['WinShares'], # aqui informamos as labels e na mesma ordem dos atributos
                                                    test_size=0.25, # informamos a porcentagem de divisão da base. Geralmente é algo entre 20% (0.20) a 35% (0.35)
                                                    random_state=0) # aqui informamos um "seed". É um valor aleatório e usado para que alguns algoritmos iniciem de forma aleatória a sua divisão.

"""# **Preparação dos dados**"""

# convertendo todas as colunas que são do tipo texto
# o handle_unknown é usado porque podem existir valores que só existem no X_test e não no X_train
# e queremos que a conversão não retorne em um erro caso isto aconteça
encoder_df1 = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1).fit(X_train)

# convertendo X_train e X_test para valores numéricos.
# não usamos o fit_transform porque precisaremos dos encoders novamente para X_test e y_test
X_train = encoder_df1.transform(X_train)
X_test = encoder_df1.transform(X_test)

imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')

)

X_train = imp_mean.fit_transform(X_train)
X_test = imp_mean.transform(X_test)

"""# **Treinamento**"""

modelo_knn = KNeighborsRegressor().fit(X_train, y_train)
modelo_knn.score(X_test, y_test)

modelo_lr = LinearRegression().fit(X_train, y_train)
modelo_lr.score(X_test, y_test)

modelo_svm = SVR().fit(X_train, y_train)
modelo_svm.score(X_test, y_test)

"""# **Mostrando as previsões**"""

df_test = pd.DataFrame(X_test) 
df_test['Quality_Real'] = y_test.values
df_test['Quality_Predicao_KNN'] = modelo_knn.predict(X_test)
df_test['Quality_Predicao_Linear'] = modelo_lr.predict(X_test)
df_test['Quality_Predicao_SVM'] = modelo_svm.predict(X_test)
df_test